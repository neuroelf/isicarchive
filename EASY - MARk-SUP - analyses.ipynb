{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for achieving expert agreement of clinical images: the Medical Annotation and Review of Superpixels (MARk-SUP)\n",
    "## Web-based expert annotation platform piloted for expert annotation and quantitative analysis of dermoscopic features\n",
    "\n",
    "### *Online supplementary Jupyter notebook with analyses*\n",
    "\n",
    "The cells in this notebook can be run in succession, or, as\n",
    "desired, cell-by-cell.\n",
    "\n",
    "**Please note:** the first two cells **MUST** be run in order\n",
    "to establish the necessary variables in the workspace of the\n",
    "python kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import imageio\n",
    "import numpy\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from isicarchive import font, func, imfunc\n",
    "from isicarchive.api import IsicApi\n",
    "\n",
    "# function for mean and sample STD\n",
    "def mean_std(a:list, is_sample:bool=True):\n",
    "    ddof = 1 if is_sample else 0\n",
    "    return (numpy.mean(a), numpy.std(a, ddof=ddof))\n",
    "\n",
    "# set to True if you would like (more) details in the print-out\n",
    "print_details = False\n",
    "print_fine_details = False\n",
    "heatmap_mix_colors = False\n",
    "heatmap_underlay_gray = 0.8\n",
    "heatmap_resize_output = 4096\n",
    "heatmap_legend_font_size = 144.0\n",
    "overlap_compute_smcc = False\n",
    "overlap_colormap = 'Greys'\n",
    "overlap_blow_up = 24\n",
    "calibri = font.Font('calibri')\n",
    "\n",
    "# please change the username accordingly!\n",
    "username = 'weberj3@mskcc.org'\n",
    "\n",
    "# root folder for all ISIC related data\n",
    "doc_folder = 'Z:\\\\10.Imaging Informatics\\\\'\n",
    "\n",
    "# cache folder\n",
    "cache_folder = doc_folder + 'ISIC' + os.sep + 'cache'\n",
    "\n",
    "# show URL requests (for debugging purposes only!)\n",
    "debug = False\n",
    "\n",
    "# instantiate API object\n",
    "api = IsicApi(username, cache_folder=cache_folder, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study folder\n",
    "study_folder = doc_folder + 'EASY' + os.sep + 'PILOT' + os.sep\n",
    "\n",
    "# load study and data\n",
    "study = api.study('ISIC Annotation Study - All Features')\n",
    "study.cache_image_data()\n",
    "study.load_annotations()\n",
    "\n",
    "# load meta data\n",
    "meta_data_url = ('https://raw.githubusercontent.com/neuroelf/' +\n",
    "    'isicarchive/master/data/EASY_pilot_diagnoses.csv')\n",
    "study.load_meta_data(meta_data_url, list_to_dict=True,\n",
    "    dict_key='name', extract_key=['diagnosis', 'exemplar'])\n",
    "\n",
    "# and create a dictionary mapping diagnosis to a list of images\n",
    "diag_images = dict()\n",
    "for (name, diag) in study.meta_data['diagnosis'].items():\n",
    "    if not diag in diag_images:\n",
    "        diag_images[diag] = []\n",
    "    diag_images[diag].append(name)\n",
    "\n",
    "# same for exemplar features\n",
    "exem_images = dict()\n",
    "for (name, exemplar) in study.meta_data['exemplar'].items():\n",
    "    if not exemplar:\n",
    "        continue\n",
    "    if not exemplar in exem_images:\n",
    "        exem_images[exemplar] = []\n",
    "    exem_images[exemplar].append(name)\n",
    "\n",
    "# select only from users that completed the study\n",
    "num_images = len(study.images)\n",
    "users = [u for (u,c) in study.user_completion.items() if c==num_images]\n",
    "study.select_annotations(users=users)\n",
    "total_features_annotations = sum(\n",
    "    [len(a.features) for a in study.annotation_selection.values()])\n",
    "selected_features = dict()\n",
    "for annotation in study.annotation_selection.values():\n",
    "    for feature in annotation.features:\n",
    "        selected_features[feature] = True\n",
    "features_list = sorted(selected_features.keys())\n",
    "category_list = sorted(list(set([v.split(' : ')[0] for v in features_list])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional cell (for heatmaps and overlap statistics; the\n",
    "results of which are needed for some cells below, but the computation\n",
    "takes a lot of time, so it's split out into a separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmaps with default settings (if not yet done)\n",
    "study_stats_file = study_folder + 'heatmap_stats.json.gz'\n",
    "if not os.path.exists(study_stats_file):\n",
    "    study_stats = study.image_heatmaps(study_folder, users=users,\n",
    "        mix_colors=heatmap_mix_colors, underlay_gray=heatmap_underlay_gray, \n",
    "        resize_output=heatmap_resize_output, font_size=heatmap_legend_font_size)\n",
    "else:\n",
    "    study_stats = func.gzip_load_var(study_stats_file)\n",
    "\n",
    "# create overlap stats\n",
    "overlap_stats_file = study_folder + 'overlap_features.npz'\n",
    "if not os.path.exists(overlap_stats_file):\n",
    "    overlap_stats = study.overlap_stats(users=users, \n",
    "        compute_smcc=overlap_compute_smcc)\n",
    "    overlap_features_dice = overlap_stats[4]\n",
    "    overlap_feat_cat_dice = overlap_stats[6]\n",
    "    overlap_category_dice = overlap_stats[8]\n",
    "    if overlap_compute_smcc:\n",
    "        overlap_features_smcc = overlap_stats[10]\n",
    "        overlap_feat_cat_smcc = overlap_stats[12]\n",
    "        overlap_category_smcc = overlap_stats[14]\n",
    "        numpy.savez(overlap_stats_file,\n",
    "            overlap_features_dice=overlap_features_dice,\n",
    "            overlap_feat_cat_dice=overlap_feat_cat_dice,\n",
    "            overlap_category_dice=overlap_category_dice,\n",
    "            overlap_features_smcc=overlap_features_smcc,\n",
    "            overlap_feat_cat_smcc=overlap_feat_cat_smcc,\n",
    "            overlap_category_smcc=overlap_category_smcc)\n",
    "    else:\n",
    "        numpy.savez(overlap_stats_file,\n",
    "            overlap_features_dice=overlap_features_dice,\n",
    "            overlap_feat_cat_dice=overlap_feat_cat_dice,\n",
    "            overlap_category_dice=overlap_category_dice)\n",
    "overlap_stats = numpy.load(overlap_stats_file)\n",
    "overlap_features_dice = overlap_stats.get('overlap_features_dice')\n",
    "overlap_feat_cat_dice = overlap_stats.get('overlap_feat_cat_dice')\n",
    "overlap_category_dice = overlap_stats.get('overlap_category_dice')\n",
    "if 'overlap_features_smcc' in overlap_stats.keys():\n",
    "    overlap_compute_smcc = True\n",
    "if overlap_compute_smcc:\n",
    "    try:\n",
    "        overlap_features_smcc = overlap_stats.get('overlap_features_smcc')\n",
    "        overlap_feat_cat_smcc = overlap_stats.get('overlap_feat_cat_smcc')\n",
    "        overlap_category_smcc = overlap_stats.get('overlap_category_smcc')\n",
    "    except:\n",
    "        overlap_compute_smcc = False\n",
    "\n",
    "# select those annotations, and gather basic statistics\n",
    "selected_annotations = study.select_annotations(users=users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives some basic statistics of the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{1:d} readers annotated the {2:d} dermoscopic images.'.format(\n",
    "    study.name, len(users), len(study.images)))\n",
    "if print_details:\n",
    "    print(' ... per diagnosis:')\n",
    "    for (diag, image_list) in diag_images.items():\n",
    "        print(' - {0:d} in {1:s}'.format(len(image_list), diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Out of {0:d} offered features, {1:d} were selected at least once.'.format(\n",
    "    len(study.features), len(selected_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Each reader annotated an average of {0:.1f} features per lesion.'.format(\n",
    "    float(total_features_annotations) / len(study.annotation_selection)))\n",
    "print('In total, {0:d} feature annotations (markups) were made.'.format(\n",
    "    total_features_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional computations for complex statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute minimum and maximum number of features (per image/diagnosis)\n",
    "(fmin, fmin_std, fmin_diag) = (85.0, 0.0, None)\n",
    "(fmax, fmax_std, fmax_diag) = (0.0, 0.0, None)\n",
    "if print_details:\n",
    "    print('On average, images with diagnosis ...')\n",
    "for diagnosis in sorted(diag_images.keys()):\n",
    "    study.select_annotations(images=diag_images[diagnosis], users=users)\n",
    "    ao = [a for a in study.annotation_selection.values()]\n",
    "    fn = [None] * len(ao)\n",
    "    for (idx,a) in enumerate(ao):\n",
    "        fn[idx] = len(a.features)\n",
    "    (m,s) = mean_std(fn)\n",
    "    if m < fmin:\n",
    "        (fmin, fmin_std, fmin_diag) = (m, s, diagnosis)\n",
    "    if m > fmax:\n",
    "        (fmax, fmax_std, fmax_diag) = (m, s, diagnosis)\n",
    "    if print_details:\n",
    "        print(' - \"{0:s}\" have {1:.2f} ± {2:.2f} annotations.'.format(\n",
    "            diagnosis, m, s))\n",
    "print('Based on the diagnosis, the number of features per image varied from')\n",
    "print('{0:.2f} (±{1:.2f}) for {2:s} to {3:.2f} (±{4:.2f}) for {5:s}.'.format(\n",
    "    fmin, fmin_std, fmin_diag, fmax, fmax_std, fmax_diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image, test whether all five raters agreed on one feature\n",
    "image_agreed = [False] * len(study.images)\n",
    "image_agreed_features = [[] for l in range(len(study.images))]\n",
    "image_orphan_features = []\n",
    "for (idx, image) in enumerate(study.images):\n",
    "    image_id = image['_id']\n",
    "    study.select_annotations(images=[image_id], users=users)\n",
    "    ao = [a for a in study.annotation_selection.values()]\n",
    "    for feature in ao[0].features.keys():\n",
    "        agreed = [False] * len(ao)\n",
    "        feature_syns = api.feature_synonyms(feature)\n",
    "        for (aidx, a) in enumerate(ao):\n",
    "            for f in feature_syns:\n",
    "                if f in a.features:\n",
    "                    agreed[aidx] = True\n",
    "                    break\n",
    "        if all(agreed):\n",
    "            image_agreed[idx] = True\n",
    "            image_agreed_features[idx].append(feature)\n",
    "    for (aidx1, a1) in enumerate(ao):\n",
    "        for f1 in a1.features.keys():\n",
    "            feature_syns = api.feature_synonyms(feature)\n",
    "total_agreements = 0\n",
    "if print_details:\n",
    "    print('Feature-in-image agreements:')\n",
    "for (image, a, af) in zip(study.images, image_agreed, image_agreed_features):\n",
    "    if a:\n",
    "        total_agreements += len(af)\n",
    "        image_name = image['name']\n",
    "        image_diag = study.meta_data['diagnosis'][image_name]\n",
    "        if print_details:\n",
    "            print(' - {0:s} ({1:s}): {2:s}'.format(image_name,\n",
    "                image_diag, ', '.join(af)))\n",
    "print('There were a total of {0:d} feature-in-image agreements.'.format(\n",
    "    total_agreements))\n",
    "print(('These were reached in a total of {0:d} images.').format(\n",
    "    numpy.sum(image_agreed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count features, including orphans\n",
    "feature_markups = dict()\n",
    "feature_in_images = dict()\n",
    "agreed_3 = dict()\n",
    "agreed_4 = dict()\n",
    "agreed_5 = dict()\n",
    "for feat_name in features_list:\n",
    "    feature_markups[feat_name] = 0\n",
    "    feature_in_images[feat_name] = set()\n",
    "    agreed_3[feat_name] = []\n",
    "    agreed_4[feat_name] = []\n",
    "    agreed_5[feat_name] = []\n",
    "orphans = 0\n",
    "orphan_image_features = []\n",
    "orphan_features = dict()\n",
    "for (idx, image) in enumerate(study.images):\n",
    "    image_id = image['_id']\n",
    "    image_name = image['name']\n",
    "    study.select_annotations(images=[image_id], users=users)\n",
    "    ao = [a for a in study.annotation_selection.values()]\n",
    "    for feature in features_list:\n",
    "        fcount = 0\n",
    "        for a in ao:\n",
    "            if feature in a.features:\n",
    "                fcount += 1\n",
    "        if fcount > 2:\n",
    "            agreed_3[feature].append(image_name)\n",
    "        if fcount > 3:\n",
    "            agreed_4[feature].append(image_name)\n",
    "        if fcount > 4:\n",
    "            agreed_5[feature].append(image_name)\n",
    "    for (aidx, a) in enumerate(ao):\n",
    "        for feature in a.features:\n",
    "            feature_markups[feature] += 1\n",
    "            feature_in_images[feature].add(image_name)\n",
    "            is_orphan = True\n",
    "            feature_syns = api.feature_synonyms(feature)\n",
    "            for (aidx2, a2) in enumerate(ao):\n",
    "                if aidx == aidx2:\n",
    "                    continue\n",
    "                for feature2 in a2.features:\n",
    "                    if feature2 in feature_syns:\n",
    "                        is_orphan = False\n",
    "                        break\n",
    "                if not is_orphan:\n",
    "                    break\n",
    "            if is_orphan:\n",
    "                orphans += 1\n",
    "                orphan_image_features.append(feature + ' in ' + \n",
    "                    image_name + ' by ' + a.user['name'])\n",
    "                if not feature_syns[0] in orphan_features:\n",
    "                    orphan_features[feature_syns[0]] = 0\n",
    "                orphan_features[feature_syns[0]] += 1\n",
    "print('An orphan observation of a feature occurred {0:d} times.'.format(\n",
    "    orphans))\n",
    "if print_details:\n",
    "    print('{0:d} orphan features were selected:'.format(orphans))\n",
    "    for orphaned in orphan_image_features:\n",
    "        print(' - ' + orphaned)\n",
    "    print('Presenting by list of features (collapsing synonyms):')\n",
    "    for feature in sorted(orphan_features.keys()):\n",
    "        print(' - {0:-2d} times \"{1:s}\"'.format(\n",
    "            orphan_features[feature], feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many exemplar feature annotations for each image with an exemplar?\n",
    "total_exemplar_images = 0\n",
    "total_found_direct = 0\n",
    "total_found_list = []\n",
    "total_found_category = 0\n",
    "total_found_specific = 0\n",
    "for exemplar in sorted(exem_images.keys()):\n",
    "    images = exem_images[exemplar]\n",
    "    if print_details:\n",
    "        print('Exemplar \"{0:s}\" with {1:d} images:'.format(\n",
    "            exemplar, len(images)))\n",
    "    for image in images:\n",
    "        total_exemplar_images += 1\n",
    "        imag_diag = study.meta_data['diagnosis'][image]\n",
    "        study.select_annotations(images=[image], users=users)\n",
    "        ao = [a for a in study.annotation_selection.values()]\n",
    "        found_direct = 0\n",
    "        found_category = 0\n",
    "        found_specific = 0\n",
    "        for (idx,a) in enumerate(ao):\n",
    "            if exemplar in a.features:\n",
    "                found_direct += 1\n",
    "                found_category += 1\n",
    "                found_specific +=1\n",
    "                continue\n",
    "            found_at_all = False\n",
    "            for feature in a.features:\n",
    "                feature = feature.split(' : ')\n",
    "                if exemplar[0:len(feature[0])] == feature[0]:\n",
    "                    found_category += 1\n",
    "                    found_at_all = True\n",
    "                if feature[-1] in exemplar:\n",
    "                    found_specific += 1\n",
    "                    found_at_all = True\n",
    "                if found_at_all:\n",
    "                    break\n",
    "        if found_direct == len(ao):\n",
    "            total_found_direct += 1\n",
    "            total_found_list.append(image)\n",
    "        if found_specific == len(ao):\n",
    "            total_found_specific += 1\n",
    "        if found_category == len(ao):\n",
    "            total_found_category += 1\n",
    "        if print_details:\n",
    "            print((' - {0:s} ({1:s}) has {2:d} annotations; ' +\n",
    "                   '{3:d}, {4:d}, and {5:d} with the full, category, ' +\n",
    "                   'and specific exemplar').format(image, imag_diag,\n",
    "                    len(ao), found_direct, found_category, found_specific))\n",
    "print('Out of {0:d} images with an exemplar, all readers identified this'.format(\n",
    "    total_exemplar_images))\n",
    "print('feature {0:d} times, with {1:d} category and {2:d} specific hits.'.format(\n",
    "    total_found_direct, total_found_category, total_found_specific))\n",
    "if print_details:\n",
    "    print('Direct hits:')\n",
    "    for image in total_found_list:\n",
    "        print(' - {0:s} ({1:s}, {2:s})'.format(image,\n",
    "            study.meta_data['diagnosis'][image],\n",
    "            study.meta_data['exemplar'][image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute the superpixel-wise agreement\n",
    "# (i.e. where 2 or more readers all agreed or any reader\n",
    "# disagreed with the feature in the same superpixel)\n",
    "if print_details:\n",
    "    print('Per-image super-pixel agreement:')\n",
    "image_sp_stats = dict()\n",
    "total_agreed = 0\n",
    "total_disagreed = 0\n",
    "sp_agreed_features = dict()\n",
    "for (image_name, image_stats) in study_stats.items():\n",
    "    orphans = 0\n",
    "    agreed = 0\n",
    "    disagreed = 0\n",
    "    for (spidx, sp_stats) in image_stats['sp'].items():\n",
    "        sp_keys = list(sp_stats.keys())\n",
    "        for sp_key in sp_keys:\n",
    "            if not sp_key in sp_agreed_features:\n",
    "                sp_agreed_features[sp_key] = {\n",
    "                    'agreed': 0, 'disagreed': 0, 'overlap': dict()\n",
    "                }\n",
    "        if len(sp_keys) == 1:\n",
    "            if len(sp_stats[sp_keys[0]]) == 1:\n",
    "                orphans += 1\n",
    "            else:\n",
    "                agreed += 1\n",
    "                sp_agreed_features[sp_keys[0]]['agreed'] += 1\n",
    "        else:\n",
    "            disagreed += 1\n",
    "            for sp_key in sp_keys:\n",
    "                sp_agreed_features[sp_key]['disagreed'] += 1\n",
    "                for o_key in sp_keys:\n",
    "                    if sp_key != o_key:\n",
    "                        if not o_key in sp_agreed_features[sp_key]['overlap']:\n",
    "                            sp_agreed_features[sp_key]['overlap'][o_key] = 0\n",
    "                        sp_agreed_features[sp_key]['overlap'][o_key] += 1\n",
    "    total_agreed += agreed\n",
    "    total_disagreed += disagreed\n",
    "    image_sp_stats[image_name] = {\n",
    "        'orphans': orphans, 'agreed': agreed, 'disagreed': disagreed}\n",
    "    if print_details:\n",
    "        print(' - {0:s} {1:-4.1f}% agreed ({2:d} SPs, w/o orphans; {3:s})'.format(\n",
    "            image_name, 100.0 * float(agreed) / float(agreed + disagreed),\n",
    "            agreed + disagreed, study.meta_data['diagnosis'][image_name]))\n",
    "print('Total agreement {0:-4.1f}% ({1:d} out of {2:d} total superpixels)'.format(\n",
    "    100.0 * float(total_agreed) / float(total_agreed + total_disagreed),\n",
    "    total_agreed, total_agreed + total_disagreed))\n",
    "if print_details:\n",
    "    print('Agreement by feature:')\n",
    "    for sp_key in sorted(sp_agreed_features.keys()):\n",
    "        sp_agreement = sp_agreed_features[sp_key]\n",
    "        print(' - {0:-4.1f}% agreement for {1:s} ({2:d} SPs)'.format(\n",
    "            100.0 * float(sp_agreement['agreed']) /\n",
    "            float(sp_agreement['agreed'] + sp_agreement['disagreed']),\n",
    "            sp_key, sp_agreement['agreed'] + sp_agreement['disagreed']))\n",
    "        if print_fine_details:\n",
    "            for o_key in sorted(sp_agreement['overlap'].keys()):\n",
    "                print('   - {0:-3d} overlapped with {1:s}'.format(\n",
    "                    sp_agreement['overlap'][o_key], o_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superpixel explanation (image)\n",
    "\n",
    "Superpixels are an automatic segmentation performed by the ISIC Archive,\n",
    "which then allows annotators (readers) to select specific parcels of an\n",
    "image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap with default settings (User: Liopyris)\n",
    "sample_image = api.image('ISIC_0016094')\n",
    "sample_image.load_image_data()\n",
    "sample_data = sample_image.data\n",
    "sample_image.mark_superpixels()\n",
    "sp_data = sample_image.data\n",
    "sample_image.clear_data()\n",
    "(heatmap, stats) = study.image_heatmap('ISIC_0016094',\n",
    "    mix_colors=False,underlay_gray=0.8,users=['578e64b09fc3c10d6fd12e4f'])\n",
    "sp_image = imfunc.image_mix(sp_data, heatmap)\n",
    "sp_comparison = numpy.concatenate((sample_data, sp_image), axis=1)\n",
    "api.write_image(sp_comparison, study_folder + 'ISIC_0016094+hm_w_sp.png')\n",
    "api.show_image_in_notebook(sp_comparison, max_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n",
    "\n",
    "The images below show (1) a demonstration of people using\n",
    "different terms for the same feature, (2) people agreeing\n",
    "agreeing on one specific term (but not others), and (3)\n",
    "everybody agreeing on the (singular) term in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show three images (heatmaps) of the study with agreement examples\n",
    "print('(1)')\n",
    "(heatmap, heatmap_stats) = study.image_heatmap('ISIC_0015549',\n",
    "    users=users, underlay_gray=heatmap_underlay_gray,\n",
    "    mix_colors=heatmap_mix_colors, resize_output=4096)\n",
    "api.show_image_in_notebook(heatmap, max_size=1024)\n",
    "print('(2)')\n",
    "(heatmap, heatmap_stats) = study.image_heatmap('ISIC_0016094',\n",
    "    users=users, underlay_gray=heatmap_underlay_gray,\n",
    "    mix_colors=heatmap_mix_colors, resize_output=4096)\n",
    "api.show_image_in_notebook(heatmap, max_size=1024)\n",
    "print('(3)')\n",
    "(heatmap, heatmap_stats) = study.image_heatmap('ISIC_0016128',\n",
    "    users=users, underlay_gray=heatmap_underlay_gray,\n",
    "    mix_colors=heatmap_mix_colors, resize_output=4096)\n",
    "api.show_image_in_notebook(heatmap, max_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature overlap/confusion figure\n",
    "\n",
    "The image prepared with the code below shows the matrix of feature overlap,\n",
    "and thus confusability; collapsed across images/diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the color lookup map\n",
    "color_norm = mpl.colors.Normalize(vmin=0.0, vmax=1.0)\n",
    "color_map = cm.ScalarMappable(norm=color_norm, cmap=overlap_colormap)\n",
    "\n",
    "overlap_stats = numpy.load(overlap_stats_file)\n",
    "overlap_features_dice = overlap_stats.get('overlap_features_dice')\n",
    "overlap_feat_cat_dice = overlap_stats.get('overlap_feat_cat_dice')\n",
    "overlap_category_dice = overlap_stats.get('overlap_category_dice')\n",
    "\n",
    "# process data prior to creating the image\n",
    "white_val = numpy.uint8(255)\n",
    "overlap_features_null = numpy.isnan(overlap_features_dice)\n",
    "overlap_feat_cat_null = numpy.isnan(overlap_feat_cat_dice)\n",
    "overlap_category_null = numpy.isnan(overlap_category_dice)\n",
    "overlap_features_dice[overlap_features_null] = 0\n",
    "overlap_feat_cat_dice[overlap_feat_cat_null] = 0\n",
    "overlap_category_dice[overlap_category_null] = 0\n",
    "overlap_features_rgb = numpy.trunc(255.0 *\n",
    "    color_map.to_rgba(overlap_features_dice[:,:,0])[:,:,0:3]).astype(numpy.uint8)\n",
    "overlap_features_rgb[numpy.repeat(overlap_features_null[:,:,0:1], 3, axis=2)] = 255\n",
    "overlap_feat_cat_rgb = numpy.trunc(255.0 *\n",
    "    color_map.to_rgba(overlap_feat_cat_dice[:,:,0])[:,:,0:3]).astype(numpy.uint8)\n",
    "overlap_feat_cat_rgb[numpy.repeat(overlap_feat_cat_null[:,:,0:1], 3, axis=2)] = 255\n",
    "overlap_category_rgb = numpy.trunc(255.0 *\n",
    "    color_map.to_rgba(overlap_category_dice[:,:,0])[:,:,0:3]).astype(numpy.uint8)\n",
    "overlap_category_rgb[numpy.repeat(overlap_category_null[:,:,0:1], 3, axis=2)] = 255\n",
    "\n",
    "# add spacers spacers\n",
    "f1v = white_val * numpy.ones((overlap_category_dice.shape[0], 2, 3,),\n",
    "    dtype=numpy.uint8)\n",
    "f2v = white_val * numpy.ones((overlap_features_dice.shape[0], 2, 3,),\n",
    "    dtype=numpy.uint8)\n",
    "blank_space = white_val * numpy.ones(\n",
    "    (overlap_category_dice.shape[0], overlap_features_dice.shape[0], 3,),\n",
    "    dtype=numpy.uint8)\n",
    "top_image = numpy.concatenate(\n",
    "    (overlap_category_rgb, f1v, blank_space), axis=1)\n",
    "bottom_image = numpy.concatenate(\n",
    "    (overlap_feat_cat_rgb, f2v, overlap_features_rgb), axis=1)\n",
    "f1h = white_val * numpy.ones((2, top_image.shape[1], 3,), dtype=numpy.uint8)\n",
    "full_image = numpy.concatenate((top_image, f1h, bottom_image), axis=0)\n",
    "\n",
    "# blow up by factor 24 (for text attribution)\n",
    "detail_blow_up = 5 * overlap_blow_up\n",
    "detail_image = numpy.repeat(numpy.repeat(\n",
    "    full_image[0:len(category_list), 0:len(category_list), :],\n",
    "    detail_blow_up, axis=0), detail_blow_up, axis=1)\n",
    "full_image = numpy.repeat(numpy.repeat(full_image, overlap_blow_up, axis=0),\n",
    "    overlap_blow_up, axis=1)\n",
    "\n",
    "# determine line positions and paint lines\n",
    "line_pos = []\n",
    "cat_name = category_list[0]\n",
    "for (idx,feature) in enumerate(features_list):\n",
    "    if not cat_name in feature:\n",
    "        line_pos.append(idx)\n",
    "        cat_name = feature.split(' : ')[0]\n",
    "line_offset = overlap_blow_up * (len(category_list) + 2) - 1\n",
    "for lp in line_pos:\n",
    "    line_xy = line_offset + overlap_blow_up * lp\n",
    "    full_image[line_xy:line_xy+2,:,:] = 0\n",
    "    full_image[line_offset:, line_xy:line_xy+2,:] = 0\n",
    "\n",
    "# create text images\n",
    "category_text = calibri.set_line(category_list, fsize=overlap_blow_up-1)\n",
    "category_text_length = [v for v in map(lambda x: x.shape[1], category_text)]\n",
    "detail_text = calibri.set_line(category_list, fsize=(3*overlap_blow_up))\n",
    "detail_text_length = [v for v in map(lambda x: x.shape[1], detail_text)]\n",
    "detail_max_length = max(detail_text_length)\n",
    "features_text = calibri.set_line(features_list, fsize=overlap_blow_up-1)\n",
    "features_text_length = [v for v in map(lambda x: x.shape[1], features_text)]\n",
    "features_max_length = max(features_text_length)\n",
    "detail_timage_x = detail_max_length + 4 * overlap_blow_up\n",
    "detail_timage = white_val * numpy.ones(\n",
    "    (detail_image.shape[0], detail_timage_x, 3, ), dtype=numpy.uint8)\n",
    "text_image_y = overlap_blow_up * (2 + len(category_text) + len(features_text))\n",
    "text_image_x = features_max_length + 2 * overlap_blow_up\n",
    "text_image = white_val * numpy.ones(\n",
    "    (text_image_y, text_image_x, 3,), dtype=numpy.uint8)\n",
    "for (idx, cat_name) in enumerate(category_list):\n",
    "    line_w = category_text_length[idx]\n",
    "    line_text = white_val - numpy.repeat(\n",
    "        category_text[idx].reshape((overlap_blow_up, line_w, 1,)), 3, axis=2)\n",
    "    line_xy = idx * overlap_blow_up\n",
    "    line_x = overlap_blow_up + (features_max_length - line_w)\n",
    "    text_image[line_xy:line_xy+overlap_blow_up,line_x:line_x+line_w,:] = line_text\n",
    "    line_w = detail_text_length[idx]\n",
    "    line_text = white_val - numpy.repeat(detail_text[idx].reshape(\n",
    "        (detail_text[idx].shape[0], line_w, 1,)), 3, axis=2)\n",
    "    line_xy = (idx * 5 + 1) * overlap_blow_up\n",
    "    line_x = 2 * overlap_blow_up + (detail_max_length - line_w)\n",
    "    detail_timage[line_xy:line_xy+line_text.shape[0],line_x:line_x+line_w,:] = line_text\n",
    "for (idx, feat_name) in enumerate(features_list):\n",
    "    line_w = features_text_length[idx]\n",
    "    line_text = white_val - numpy.repeat(\n",
    "        features_text[idx].reshape((overlap_blow_up, line_w, 1,)), 3, axis=2)\n",
    "    line_xy = (len(category_list) + 2 + idx) * overlap_blow_up\n",
    "    line_x = overlap_blow_up + (features_max_length - line_w)\n",
    "    text_image[line_xy:line_xy+overlap_blow_up, line_x:line_x+line_w,:] = line_text\n",
    "\n",
    "# put everything together\n",
    "detail_image = numpy.concatenate((detail_timage, detail_image), axis=1)\n",
    "full_image = numpy.concatenate(\n",
    "    (full_image, imfunc.image_rotate(text_image, 'left')), axis=0)\n",
    "text_filler = white_val * numpy.ones((text_image_x, text_image_x, 3,),\n",
    "    dtype=numpy.uint8)\n",
    "text_image = numpy.concatenate((text_image, text_filler), axis=0)\n",
    "full_image = numpy.concatenate((text_image, full_image), axis=1)\n",
    "\n",
    "# write and show image\n",
    "api.write_image(detail_image, study_folder + 'EASY_PILOT_DICE_confusion_markup_cat.png')\n",
    "api.write_image(full_image, study_folder + 'EASY_PILOT_DICE_confusion_markup_full.png')\n",
    "api.show_image_in_notebook(detail_image, max_size=1024)\n",
    "api.show_image_in_notebook(full_image, max_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary data\n",
    "\n",
    "## Table of ISIC images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "S1_study_images = pd.DataFrame.from_dict({\n",
    "    'ISIC name': [img['name'] for img in study.images],\n",
    "    'ISIC imageId': [img['_id'] for img in study.images],\n",
    "    'diagnosis': [study.meta_data['diagnosis'][img['name']] for img in study.images],\n",
    "    'exemplar feature': [study.meta_data['exemplar'][img['name']] for img in study.images],\n",
    "})\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(S1_study_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables of dermoscopic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features_list = sorted(list([f['id'] for f in study.features]))\n",
    "S2_dermoscopic_features = pd.DataFrame.from_dict({\n",
    "    'feature name': full_features_list,\n",
    "    'used (times)': [feature_markups[f] if f in feature_markups else 0\n",
    "                     for f in full_features_list],\n",
    "    'in images': [len(feature_in_images[f]) if f in feature_in_images else 0\n",
    "                  for f in full_features_list],\n",
    "    'agreed 3x': [len(agreed_3[f]) if f in agreed_3 else 0\n",
    "                  for f in full_features_list],\n",
    "    'agreed 4x': [len(agreed_4[f]) if f in agreed_4 else 0\n",
    "                  for f in full_features_list],\n",
    "    'agreed 5x': [len(agreed_5[f]) if f in agreed_5 else 0\n",
    "                  for f in full_features_list],\n",
    "})\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(S2_dermoscopic_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
